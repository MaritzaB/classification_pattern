{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea 7. Experimentacion\n",
    "\n",
    "Esta tarea consiste en llevar a cabo la experimentación de diversos\n",
    "clasificadores sobre una variedad de datasets. Se reportarán los resultados y,\n",
    "posteriormente cuando tengamos la clase de significancia estadística, haremos el\n",
    "análisis de los clasificadores y si hay un ganador cuál de ellos sería.\n",
    "\n",
    "Las instrucciones se detallan a continuación:\n",
    "\n",
    "1. Buscar varios datasets (7, 8, 10, etc.) que cumplan con una condición: deben\n",
    "   tener alguna característica en común. ¿Qué característica en común? Hay\n",
    "   varias cosas que pueden satisfacer esta condición, por ejemplo, que traten el\n",
    "   mismo problema (e.g. diagnósticos médicos aunque sean de diferente\n",
    "   enfermedad), que tengan muchas más variables que instancias (maldición de la\n",
    "   dimensionalidad), que todos sean de más de 3 clases, que sus clases estén\n",
    "   desbalanceadas, entre otras cosas que ustedes puedan identificar. La idea es\n",
    "   que dichos datasets puedan ser identificados en el mismo contexto.\n",
    "\n",
    "2. Llevar a cabo la clasificación, utilizando scikit-learn, aplicando los\n",
    "   clasificadores que hemos visto y algunos otros de su predilección. Para\n",
    "   realizar la clasificación, deben emplear algún método de validación cruzada\n",
    "   (k-fold cross-validation, o leave-one-out, dependiendo la cantidad de datos).\n",
    "   Asimismo, deberán reportar los resultados utilizando el balanced accuracy,\n",
    "   sensibilidad y especificidad (juntas), o bien el área bajo la curva roc (AUC\n",
    "   ROC).\n",
    "\n",
    "3. Posteriormente realizaremos alguna prueba de significancia estadística (cuyo\n",
    "   tema veremos al regreso de vacaciones), con la finalidad de conocer si\n",
    "   existen diferencias estadísticamente significativas entre los clasificadores.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datos de medidas morfológicas de animales para inferir especie o sexo.\n",
    "\n",
    "1. [Abalone dataset](https://archive-beta.ics.uci.edu/dataset/1/abalone)\n",
    "\n",
    "1. [Birds\n",
    "   bones dataset](https://www.kaggle.com/datasets/zhangjuefei/birds-bones-and-living-habits)\n",
    "\n",
    "1. [Penguins dataset](https://archive-beta.ics.uci.edu/dataset/690/palmer+penguins-3)\n",
    "\n",
    "1. [Pokemon_dataset](https://www.kaggle.com/datasets/cristobalmitchell/pokedex)\n",
    "\n",
    "1. [Sloths dataset](https://www.kaggle.com/datasets/bertiemackie/sloth-species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "abalones = pd.read_csv(\"../datasets/abalone.csv\")\n",
    "print(abalones.shape)\n",
    "abalones_ft = abalones.loc[:,['Shell_length', 'Shell_diameter', 'Height', 'Whole_weight',\n",
    "       'Shucked_weight', 'Viscera_weight', 'Shell_weight',]]\n",
    "abalones_tg = abalones[['Sex_category']]\n",
    "\n",
    "birds = pd.read_csv(\"../datasets/bird.csv\")\n",
    "print(birds.shape)\n",
    "birds_ft = birds.loc[:,['Humerus_length', 'Humerus_diameter', 'Ulna_length',\n",
    "       'Ulna_diamater', 'Femur_length', 'Femur_diameter', 'Tibiotarsus_length',\n",
    "       'Tibiotarsus_diameter', 'Tarsometatarsus_length',\n",
    "       'Tarsometatarsus_diameter',]]\n",
    "birds_tg = birds[['Species_group_category']]\n",
    "\n",
    "penguins = pd.read_csv(\"../datasets/penguins.csv\")\n",
    "print(penguins.shape)\n",
    "penguins_ft = penguins.loc[:,['Culmen_Length_mm', 'Culmen_Depth_mm', 'Flipper_Length_', 'Body_Massgr', ]]\n",
    "penguins_tg = penguins[['Species_category']]\n",
    "\n",
    "#pokemon = pd.read_csv(\"../datasets/pokemon.csv\")\n",
    "\n",
    "sloths = pd.read_csv(\"../datasets/sloth_data.csv\")\n",
    "print(sloths.shape)\n",
    "sloths_ft = sloths.loc[:,['Claw_length_cm', 'Size_cm', 'Tail_length_cm', 'Weight_kg', ]]\n",
    "sloths_tg = sloths[['Sub_specie_category']]\n",
    "\n",
    "datasets = [\n",
    "    (abalones_ft, abalones_tg),\n",
    "    (birds_ft, birds_tg),\n",
    "    (penguins_ft, penguins_tg),\n",
    "    (sloths_ft, sloths_tg)\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define modelos a utilizar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "classifiers = []\n",
    "\n",
    "lr = LogisticRegression()\n",
    "dtc = DecisionTreeClassifier()\n",
    "svc = SVC()\n",
    "knn = KNeighborsClassifier()\n",
    "rfc = RandomForestClassifier()\n",
    "gbc = GradientBoostingClassifier()\n",
    "#classifiers.append(lr)\n",
    "classifiers.append(dtc)\n",
    "classifiers.append(svc)\n",
    "classifiers.append(knn)\n",
    "classifiers.append(rfc)\n",
    "classifiers.append(gbc)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validación cruzada (Hold Out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score,f1_score\n",
    "from imblearn.metrics import sensitivity_score, specificity_score, geometric_mean_score\n",
    "\n",
    "class DatasetSplitter:\n",
    "    def __init__(self, test_size, random_state=1111):\n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "    \n",
    "    def split_datasets(self, datasets, classifiers):\n",
    "        results = []\n",
    "        for i, (data, target) in enumerate(datasets):\n",
    "            X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=self.test_size, random_state=self.random_state)\n",
    "            classifier = classifiers\n",
    "            classifier.fit(X_train, y_train.values.ravel())\n",
    "            y_pred = classifier.predict(X_test)\n",
    "            k = len(set(y_test))\n",
    "            if k==2:\n",
    "              avg = 'binary'\n",
    "              multi = 'raise'\n",
    "            else:\n",
    "              avg = 'macro'\n",
    "              multi = 'ovr'\n",
    "\n",
    "            accuracy = accuracy_score(y_test,y_pred)\n",
    "            sensitivity = sensitivity_score(y_test,y_pred, average=avg)\n",
    "            specificity = specificity_score(y_test,y_pred, average=avg)\n",
    "            f1_sc = f1_score(y_test,y_pred, average=avg)\n",
    "            gmean = geometric_mean_score(y_test,y_pred, average=avg)\n",
    "            # auc = roc_auc_score(y_test,y_pred,multi_class=multi)\n",
    "            results.append((accuracy, sensitivity, specificity, f1_sc, gmean))\n",
    "        results = pd.DataFrame(results, columns=['accuracy', 'sensitivity', 'specificity', 'f1_sc', 'gmean'])\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = DatasetSplitter(test_size=0.2, random_state=42)\n",
    "\n",
    "#for model in classifiers:\n",
    "#    metrics = splitter.split_datasets(datasets, model)\n",
    "#    print(model.__class__.__name__)\n",
    "#    print(metrics)\n",
    "\n",
    "dtc_metrics = splitter.split_datasets(datasets, classifiers[0])\n",
    "svc_metrics = splitter.split_datasets(datasets, classifiers[1])\n",
    "knn_metrics = splitter.split_datasets(datasets, classifiers[2])\n",
    "rfc_metrics = splitter.split_datasets(datasets, classifiers[3])\n",
    "gbc_metrics = splitter.split_datasets(datasets, classifiers[4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict = {\n",
    "    'Decision Tree classifier': dtc_metrics*100,\n",
    "    'Support Vector Machine Classifier': svc_metrics*100,\n",
    "    'K Nearest Neighbour Classifier': knn_metrics*100,\n",
    "    'Random Forest Classifier': rfc_metrics*100,\n",
    "    'Gradient Boosting Classifier': gbc_metrics*100\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print metrics\n",
    "\n",
    "for key, value in my_dict.items():\n",
    "       print(key, '\\n', value)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exactitud (Accuracy)\n",
    "\n",
    "La exactitud mide la proporción de predicciones correctas del modelo. Es la métrica más básica y se define como el número de predicciones correctas dividido por el número total de predicciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy metric\n",
    "for key, value in my_dict.items():\n",
    "       plt.plot(value['accuracy'], label=key)\n",
    "plt.xticks([0, 1, 2, 3, ], ['abalone', 'birds', 'penguins', 'sloths', ],\n",
    "       rotation=20)\n",
    "plt.legend()\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('dataset')\n",
    "plt.title('Metric: accuracy_score')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensitivity score\n",
    "\n",
    "Es una métrica de evaluación que se utiliza para medir la sensibilidad o la tasa de verdaderos positivos en un problema de clasificación binaria o multiclase, especialmente cuando hay un desequilibrio en la distribución de las clases en los datos de entrenamiento. La sensibilidad se refiere a la capacidad de un modelo para identificar correctamente las muestras positivas, es decir, aquellas que pertenecen a la clase minoritaria o que son relevantes para el problema.\n",
    "\n",
    "Es una medida de evaluación para abordar el problema de desequilibrio de clases en la clasificación. En comparación con otras métricas de evaluación, como la exactitud, que pueden ser engañosas cuando hay desequilibrio en la distribución de clases, el sensitivity_score proporciona una mejor medida de la capacidad del modelo para detectar correctamente las muestras positivas.\n",
    "\n",
    "Es especialmente útil en problemas en los que la clase minoritaria es más importante que la clase mayoritaria, como en la detección de enfermedades raras o en la detección de fraudes financieros, entre otros\n",
    "\n",
    "La sensibilidad macro es adecuada cuando se desea que todas las clases tengan el mismo peso, mientras que la precisión micro es útil cuando se desea dar más peso a las clases con más muestras.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot sensitivity metric\n",
    "for key, value in my_dict.items():\n",
    "       plt.plot(value['sensitivity'], label=key)\n",
    "plt.xticks([0, 1, 2, 3, ], ['abalone', 'birds', 'penguins', 'sloths', ],\n",
    "       rotation=20)\n",
    "plt.legend()\n",
    "plt.ylabel('sensitivity')\n",
    "plt.xlabel('dataset')\n",
    "plt.title('Metric: sensitivity_score')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Especificidad o tasa de verdaderos negativos (Specificity or True Negative Rate)\n",
    "\n",
    "La especificidad mide la proporción de casos negativos que son correctamente identificados. Se define como el número de verdaderos negativos dividido por la suma de verdaderos negativos y falsos positivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot specificity metric\n",
    "for key, value in my_dict.items():\n",
    "       plt.plot(value['specificity'], label=key)\n",
    "plt.xticks([0, 1, 2, 3, ], ['abalone', 'birds', 'penguins', 'sloths', ],\n",
    "       rotation=20)\n",
    "plt.legend()\n",
    "plt.ylabel('specificity')\n",
    "plt.xlabel('dataset')\n",
    "plt.title('Metric: specificity_score')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valor-F (F-Score)\n",
    "\n",
    "El valor-F es una media ponderada de la precisión y la sensibilidad. Se define\n",
    "como 2 veces la multiplicación de la precisión y la sensibilidad dividido por la\n",
    "suma de la precisión y la sensibilidad.\n",
    "\n",
    "El valor-F es una métrica que combina tanto la precisión como la sensibilidad en una sola medida. Si nuestro objetivo es encontrar un equilibrio entre la precisión y la sensibilidad, entonces el valor-F es una métrica útil a considerar. Supongamos que para el modelo A, el valor-F es del 0,85, mientras que para el modelo B es del 0,87. En este caso, el modelo B tendría un mejor valor-F, lo que sugiere que es más equilibrado en términos de precisión y sensibilidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot f1 score metric\n",
    "for key, value in my_dict.items():\n",
    "       plt.plot(value['f1_sc'], label=key)\n",
    "plt.xticks([0, 1, 2, 3, ], ['abalone', 'birds', 'penguins', 'sloths', ],\n",
    "       rotation=20)\n",
    "plt.legend()\n",
    "plt.ylabel('f1 score')\n",
    "plt.xlabel('dataset')\n",
    "plt.title('Metric: F1 score')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geometric mean score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot gmean score metric\n",
    "for key, value in my_dict.items():\n",
    "       plt.plot(value['gmean'], label=key)\n",
    "plt.xticks([0, 1, 2, 3, ], ['abalone', 'birds', 'penguins', 'sloths', ],\n",
    "       rotation=20)\n",
    "plt.legend()\n",
    "plt.ylabel('geometric mean score')\n",
    "plt.xlabel('dataset')\n",
    "plt.title('Metric: gmean_score')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Friedman test\n",
    "\n",
    "La prueba de Friedman es una técnica estadística no paramétrica que se utiliza para comparar el rendimiento de varios clasificadores en diferentes conjuntos de datos. En la práctica, es común evaluar varios modelos de aprendizaje automático en un conjunto de datos específico y comparar sus resultados utilizando diferentes métricas de evaluación.\n",
    "\n",
    "La prueba de Friedman se utiliza para determinar si existe una diferencia significativa entre los clasificadores en términos de su rendimiento medio. Esta prueba compara los rangos medios de los clasificadores en los diferentes conjuntos de datos para evaluar la hipótesis nula de que todos los clasificadores tienen el mismo rendimiento medio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import friedmanchisquare, rankdata\n",
    "import numpy as np\n",
    "\n",
    "acc_score_dict = pd.DataFrame({\n",
    "    'DTC': dtc_metrics['accuracy'],\n",
    "    'SVC': svc_metrics['accuracy'],\n",
    "    'KNN': knn_metrics['accuracy'],\n",
    "    'RFC': rfc_metrics['accuracy'],\n",
    "    'GBC': gbc_metrics['accuracy']\n",
    "})\n",
    "\n",
    "print(100-acc_score_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat, p = friedmanchisquare(*acc_score_dict.values.T)\n",
    "\n",
    "print(f'\\np-value: {p}\\n')\n",
    "print(f'\\nstats: {stat}\\n')\n",
    "\n",
    "rank = rankdata(acc_score_dict,axis=1)\n",
    "mrank = np.average(rank,axis=0)\n",
    "rank_id = np.argsort(mrank)\n",
    "rank_sort = np.sort(mrank)\n",
    "\n",
    "print(f'Tabla de ranking en algoritmos:\\n{rank}')\n",
    "print(f'\\nMean-ranks:')\n",
    "print(mrank)\n",
    "\n",
    "#print(list(acc_score_dict.keys()))\n",
    "#print('\\nClasificador\\t\\t\\tRanking')\n",
    "#for idx in rank_id:\n",
    "#    print(f'{classifiers[idx]} \\t\\t {mrank[idx]}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
