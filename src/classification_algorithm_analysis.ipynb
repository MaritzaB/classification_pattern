{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea 7. Experimentacion\n",
    "\n",
    "Esta tarea consiste en llevar a cabo la experimentación de diversos\n",
    "clasificadores sobre una variedad de datasets. Se reportarán los resultados y,\n",
    "posteriormente cuando tengamos la clase de significancia estadística, haremos el\n",
    "análisis de los clasificadores y si hay un ganador cuál de ellos sería.\n",
    "\n",
    "Las instrucciones se detallan a continuación:\n",
    "\n",
    "1. Buscar varios datasets (7, 8, 10, etc.) que cumplan con una condición: deben\n",
    "   tener alguna característica en común. ¿Qué característica en común? Hay\n",
    "   varias cosas que pueden satisfacer esta condición, por ejemplo, que traten el\n",
    "   mismo problema (e.g. diagnósticos médicos aunque sean de diferente\n",
    "   enfermedad), que tengan muchas más variables que instancias (maldición de la\n",
    "   dimensionalidad), que todos sean de más de 3 clases, que sus clases estén\n",
    "   desbalanceadas, entre otras cosas que ustedes puedan identificar. La idea es\n",
    "   que dichos datasets puedan ser identificados en el mismo contexto.\n",
    "\n",
    "2. Llevar a cabo la clasificación, utilizando scikit-learn, aplicando los\n",
    "   clasificadores que hemos visto y algunos otros de su predilección. Para\n",
    "   realizar la clasificación, deben emplear algún método de validación cruzada\n",
    "   (k-fold cross-validation, o leave-one-out, dependiendo la cantidad de datos).\n",
    "   Asimismo, deberán reportar los resultados utilizando el balanced accuracy,\n",
    "   sensibilidad y especificidad (juntas), o bien el área bajo la curva roc (AUC\n",
    "   ROC).\n",
    "\n",
    "3. Posteriormente realizaremos alguna prueba de significancia estadística (cuyo\n",
    "   tema veremos al regreso de vacaciones), con la finalidad de conocer si\n",
    "   existen diferencias estadísticamente significativas entre los clasificadores.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datos de medidas morfológicas de animales para inferir especie o sexo.\n",
    "\n",
    "1. [Abalone dataset](https://archive-beta.ics.uci.edu/dataset/1/abalone)\n",
    "\n",
    "1. [Birds\n",
    "   bones dataset](https://www.kaggle.com/datasets/zhangjuefei/birds-bones-and-living-habits)\n",
    "\n",
    "1. [Penguins dataset](https://archive-beta.ics.uci.edu/dataset/690/palmer+penguins-3)\n",
    "\n",
    "1. [Pokemon_dataset](https://www.kaggle.com/datasets/cristobalmitchell/pokedex)\n",
    "\n",
    "1. [Sloths dataset](https://www.kaggle.com/datasets/bertiemackie/sloth-species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "abalones = pd.read_csv(\"../datasets/abalone.csv\")\n",
    "print(abalones.shape)\n",
    "abalones_ft = abalones.loc[:,['Shell_length', 'Shell_diameter', 'Height', 'Whole_weight',\n",
    "       'Shucked_weight', 'Viscera_weight', 'Shell_weight',]]\n",
    "abalones_tg = abalones[['Sex_category']]\n",
    "\n",
    "birds = pd.read_csv(\"../datasets/bird.csv\")\n",
    "print(birds.shape)\n",
    "birds_ft = birds.loc[:,['Humerus_length', 'Humerus_diameter', 'Ulna_length',\n",
    "       'Ulna_diamater', 'Femur_length', 'Femur_diameter', 'Tibiotarsus_length',\n",
    "       'Tibiotarsus_diameter', 'Tarsometatarsus_length',\n",
    "       'Tarsometatarsus_diameter',]]\n",
    "birds_tg = birds[['Species_group_category']]\n",
    "\n",
    "penguins = pd.read_csv(\"../datasets/penguins.csv\")\n",
    "print(penguins.shape)\n",
    "penguins_ft = penguins.loc[:,['Culmen_Length_mm', 'Culmen_Depth_mm', 'Flipper_Length_', 'Body_Massgr', ]]\n",
    "penguins_tg = penguins[['Species_category']]\n",
    "\n",
    "#pokemon = pd.read_csv(\"../datasets/pokemon.csv\")\n",
    "\n",
    "sloths = pd.read_csv(\"../datasets/sloth_data.csv\")\n",
    "print(sloths.shape)\n",
    "sloths_ft = sloths.loc[:,['Claw_length_cm', 'Size_cm', 'Tail_length_cm', 'Weight_kg', ]]\n",
    "sloths_tg = sloths[['Sub_specie_category']]\n",
    "\n",
    "datasets = [\n",
    "    (abalones_ft, abalones_tg),\n",
    "    (birds_ft, birds_tg),\n",
    "    (penguins_ft, penguins_tg),\n",
    "    (sloths_ft, sloths_tg)\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define modelos a utilizar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "classifiers = []\n",
    "\n",
    "lr = LogisticRegression()\n",
    "dtc = DecisionTreeClassifier()\n",
    "svc = SVC()\n",
    "knn = KNeighborsClassifier()\n",
    "rfc = RandomForestClassifier()\n",
    "gbc = GradientBoostingClassifier()\n",
    "#classifiers.append(lr)\n",
    "classifiers.append(dtc)\n",
    "classifiers.append(svc)\n",
    "classifiers.append(knn)\n",
    "classifiers.append(rfc)\n",
    "classifiers.append(gbc)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validación cruzada (Hold Out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score,f1_score, roc_auc_score\n",
    "from imblearn.metrics import sensitivity_score, specificity_score, geometric_mean_score\n",
    "\n",
    "class DatasetSplitter:\n",
    "    def __init__(self, test_size, random_state=1111):\n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "    \n",
    "    def split_datasets(self, datasets, classifiers):\n",
    "        results = []\n",
    "        for i, (data, target) in enumerate(datasets):\n",
    "            X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=self.test_size, random_state=self.random_state)\n",
    "            classifier = classifiers\n",
    "            classifier.fit(X_train, y_train.values.ravel())\n",
    "            y_pred = classifier.predict(X_test)\n",
    "            k = len(set(y_test))\n",
    "            if k==2:\n",
    "              avg = 'binary'\n",
    "              multi = 'raise'\n",
    "            else:\n",
    "              avg = 'macro'\n",
    "              multi = 'ovr'\n",
    "\n",
    "            accuracy = accuracy_score(y_test,y_pred)\n",
    "            sensitivity = sensitivity_score(y_test,y_pred, average=avg)\n",
    "            specificity = specificity_score(y_test,y_pred, average=avg)\n",
    "            f1_sc = f1_score(y_test,y_pred, average=avg)\n",
    "            gmean = geometric_mean_score(y_test,y_pred, average=avg)\n",
    "            # auc = roc_auc_score(y_test,y_pred,multi_class=multi)\n",
    "            results.append((accuracy, sensitivity, specificity, f1_sc, gmean))\n",
    "        results = pd.DataFrame(results, columns=['accuracy', 'sensitivity', 'specificity', 'f1_sc', 'gmean'])\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = DatasetSplitter(test_size=0.2, random_state=42)\n",
    "\n",
    "#for model in classifiers:\n",
    "#    metrics = splitter.split_datasets(datasets, model)\n",
    "#    print(model.__class__.__name__)\n",
    "#    print(metrics)\n",
    "\n",
    "dtc_metrics = splitter.split_datasets(datasets, classifiers[0])\n",
    "svc_metrics = splitter.split_datasets(datasets, classifiers[1])\n",
    "knn_metrics = splitter.split_datasets(datasets, classifiers[2])\n",
    "rfc_metrics = splitter.split_datasets(datasets, classifiers[3])\n",
    "gbc_metrics = splitter.split_datasets(datasets, classifiers[4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Decision Tree Classifier:\\n', dtc_metrics)\n",
    "print('Support Vector Machine:\\n', svc_metrics)\n",
    "print('K Neighbours Classifier:\\n', knn_metrics)\n",
    "print('Random Forest Classifier:\\n', rfc_metrics)\n",
    "print('Gradient Boosting Classifier:\\n', gbc_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict = {\n",
    "    'Decision Tree classifier': dtc_metrics,\n",
    "    'Support Vector Machine Classifier': svc_metrics,\n",
    "    'K Nearest Neighbour Classifier': knn_metrics,\n",
    "    'Random Forest Classifier': rfc_metrics,\n",
    "    'Gradient Boosting Classifier': gbc_metrics\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy metric\n",
    "for key, value in my_dict.items():\n",
    "       nth_key = key\n",
    "       nth_value = value\n",
    "       plt.plot(nth_value['accuracy'], label=nth_key)\n",
    "plt.xticks([0, 1, 2, 3, ], ['abalone', 'birds', 'penguins', 'sloths', ],\n",
    "       rotation=20)\n",
    "plt.legend()\n",
    "plt.title('Metric: accuracy_score')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prueba de Mann-Whitney para comparar el desempeño de diferentes clasificadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "# Assume we have two classifiers and their accuracy scores in separate lists:\n",
    "clf1_scores = [0.85, 0.92, 0.88, 0.89, 0.91]\n",
    "clf2_scores = [0.86, 0.87, 0.83, 0.90, 0.85]\n",
    "\n",
    "# Perform a Mann-Whitney test to compare the two sets of scores:\n",
    "u_statistic, p_value = mannwhitneyu(clf1_scores, clf2_scores)\n",
    "\n",
    "# Print the results:\n",
    "print(f\"U statistic: {u_statistic}\")\n",
    "print(f\"P-value: {p_value}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
